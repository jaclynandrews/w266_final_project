{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './ALL_data/'\n",
    "\n",
    "train = pd.read_csv(data_path+'Training_Data/subtaskA_data_all.csv')\n",
    "train_labels = list(pd.read_csv(data_path+'Training_Data/subtaskA_answers_all.csv', header=None, names=['id', 'label'])['label'])\n",
    "dev = pd.read_csv(data_path+'Dev_Data/subtaskA_dev_data.csv')\n",
    "dev_labels = list(pd.read_csv(data_path+'Dev_Data/subtaskA_gold_answers.csv', header=None, names=['id', 'label'])['label'])\n",
    "test = pd.read_csv(data_path+'Test_Data/subtaskA_test_data.csv')\n",
    "test_labels = list(pd.read_csv(data_path+'Test_Data/subtaskA_gold_answers.csv', header=None, names=['id', 'label'])['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_average(sent):\n",
    "\t\t\"\"\"\n",
    "\t\tCompute average word vector for a single doc/sentence.\n",
    "\t\t:param sent: list of sentence tokens\n",
    "\t\t:return:\n",
    "\t\t\tmean: float of averaging word vectors\n",
    "\t\t\"\"\"\n",
    "\t\tmean = []\n",
    "\t\tfor word in sent:\n",
    "\t\t\tif word in wv:\n",
    "\t\t\t\tmean.append(wv[word])\n",
    "\n",
    "\t\tif not mean:  # empty words\n",
    "\t\t\t# If a text is empty, return a vector of zeros.\n",
    "\t\t\treturn np.zeros(300)\n",
    "\t\telse:\n",
    "\t\t\tmean = np.array(mean).mean(axis=0)\n",
    "\t\t\treturn mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "X = []\n",
    "for sentence in train['sent0']:\n",
    "    X.append(word_average(gensim.utils.simple_preprocess(sentence)))\n",
    "for sentence in train['sent1']:\n",
    "    X.append(word_average(gensim.utils.simple_preprocess(sentence)))\n",
    "\n",
    "y = train_labels + [1-i for i in train_labels]\n",
    "\n",
    "# dev data\n",
    "X_dev = []\n",
    "for sentence in dev['sent0']:\n",
    "    X_dev.append(word_average(gensim.utils.simple_preprocess(sentence)))\n",
    "for sentence in dev['sent1']:\n",
    "    X_dev.append(word_average(gensim.utils.simple_preprocess(sentence)))\n",
    "\n",
    "y_dev = dev_labels + [1-i for i in dev_labels]\n",
    "\n",
    "\n",
    "# test data\n",
    "X_test = []\n",
    "for sentence in test['sent0']:\n",
    "    X_test.append(word_average(gensim.utils.simple_preprocess(sentence)))\n",
    "for sentence in test['sent1']:\n",
    "    X_test.append(word_average(gensim.utils.simple_preprocess(sentence)))\n",
    "\n",
    "y_test = test_labels + [1-i for i in test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effects of C regularization on sum of square weights:\n",
      "C=1e-10, SS=[1.7548612e-16]\n",
      "C=0.0001, SS=[0.00017158]\n",
      "C=0.001, SS=[0.01460161]\n",
      "C=0.01, SS=[0.64332863]\n",
      "C=0.1, SS=[9.00187405]\n",
      "C=0.2, SS=[15.16480324]\n",
      "C=0.3, SS=[19.30381696]\n",
      "C=0.4, SS=[22.28672104]\n",
      "C=0.5, SS=[24.54491173]\n",
      "C=0.6, SS=[26.32012554]\n",
      "C=0.7, SS=[27.75273617]\n",
      "C=0.8, SS=[28.93541552]\n",
      "C=0.9, SS=[29.92976753]\n",
      "C=1, SS=[30.77716233]\n",
      "C=10, SS=[40.16302273]\n",
      "\n",
      "\n",
      "Logistic Regression has an accuracy of 56.52% and f1-score of 56.52% with an optimal C value of 0.5.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_acc_f1(preds, acc, f1):\n",
    "        correct, total = 0, 0\n",
    "        for pred, label in zip(preds, y_dev):\n",
    "            if pred == label: correct += 1\n",
    "            total += 1\n",
    "        acc.append(1.0*correct/total)\n",
    "        f1.append(metrics.f1_score(y_dev, preds, average='weighted'))\n",
    "        return acc, f1\n",
    "\n",
    "def print_results(classifier, parameter, param_vals, acc, f1):\n",
    "        # use the highest f1-score to determine the optimal parameter value\n",
    "        print('{} has an accuracy of {:.2f}% and f1-score of {:.2f}% with an optimal {} value of {}.\\n'\n",
    "              .format(classifier, acc[f1.index(max(f1))]*100,max(f1)*100, parameter, param_vals[f1.index(max(f1))]))\n",
    "        \n",
    "    \n",
    "# function to fit Logistic Regression models with varying C values & compute sum of square of the weights\n",
    "def build_lr():\n",
    "    c = [1.0e-10,0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 10]\n",
    "    acc = []\n",
    "    f1 = []\n",
    "    ss = []\n",
    "    print(\"Effects of C regularization on sum of square weights:\")\n",
    "    for i in c:\n",
    "        lr = LogisticRegression(solver='liblinear', C=i)\n",
    "        lr.fit(X, y)\n",
    "        preds = lr.predict(X_dev)\n",
    "        get_acc_f1(preds, acc, f1)\n",
    "        # extract the weights from .coef_ and print their sum of squares\n",
    "        ss.append(sum(np.square(lr.coef_).T))\n",
    "        print('C={}, SS={}'.format(i,sum(np.square(lr.coef_).T)))\n",
    "    print('\\n')\n",
    "    print_results('Logistic Regression', 'C', c, acc, f1)\n",
    "build_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR accuracy = 0.5651955867602808\n",
      "model accuracy = 0.5827482447342026\n"
     ]
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION WITH PERCENTAGES - USE OPTIMAL C VALUE OF 0.5\n",
    "\n",
    "\n",
    "# lr = LogisticRegression(solver='liblinear', C=0.5)\n",
    "lr.fit(X, y)\n",
    "preds = lr.predict(X_dev)\n",
    "\n",
    "correct, total = 0, 0\n",
    "for pred, label in zip(preds, y_dev):\n",
    "    if pred == label: correct += 1\n",
    "    total += 1\n",
    "acc = 1.0*correct/total\n",
    "\n",
    "print(f'LR accuracy = {acc}')\n",
    "\n",
    "\n",
    "probs = lr.predict_proba(X_dev)\n",
    "guesses = []\n",
    "for i in range(997):\n",
    "    if (probs[i][0] > probs[i+997][0]) and y_dev[i]==0:\n",
    "        guesses.append(1)\n",
    "    elif (probs[i][0] < probs[i+997][0]) and y_dev[i]==1:\n",
    "        guesses.append(1)\n",
    "    else:\n",
    "        guesses.append(0)\n",
    "print(f'model accuracy = {sum(guesses)/997}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR accuracy = 0.559\n",
      "model accuracy = 0.615\n"
     ]
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION WITH PERCENTAGES - USE OPTIMAL C VALUE OF 0.5\n",
    "\n",
    "\n",
    "# lr = LogisticRegression(solver='liblinear', C=0.5)\n",
    "lr.fit(X, y)\n",
    "preds = lr.predict(X_test)\n",
    "\n",
    "correct, total = 0, 0\n",
    "for pred, label in zip(preds, y_test):\n",
    "    if pred == label: correct += 1\n",
    "    total += 1\n",
    "acc = 1.0*correct/total\n",
    "\n",
    "print(f'LR accuracy = {acc}')\n",
    "\n",
    "\n",
    "probs = lr.predict_proba(X_test)\n",
    "guesses = []\n",
    "for i in range(1000):\n",
    "    if (probs[i][0] > probs[i+1000][0]) and y_test[i]==0:\n",
    "        guesses.append(1)\n",
    "    elif (probs[i][0] < probs[i+1000][0]) and y_test[i]==1:\n",
    "        guesses.append(1)\n",
    "    else:\n",
    "        guesses.append(0)\n",
    "print(f'model accuracy = {sum(guesses)/1000}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
